{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ndmg on AWS Batch\n",
    "\n",
    "To run ndmg on entire datasets, we use AWS Batch. <br>\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Docker\n",
    "- An AWS account\n",
    "- Your dataset on S3\n",
    "\n",
    "## Overview\n",
    "\n",
    "Our Batch pipeline works as follows.\n",
    "\n",
    "1. Parse a BIDs-formatted dataset on s3 into a set of scans.\n",
    "2. For each scan, create a JSON file containing its subject ID, session ID, and ndmg parameters.\n",
    "3. For each JSON file, send a job to AWS.\n",
    "4. AWS, upon receiving each job, creates a new EC2 instance, grabs our Docker image, and then runs a container using the arguments specified in the JSON.\n",
    "5. The container then pulls the input data for that particular scan from S3, runs ndmg using that data, and upon completion, pushes the data back to S3 in the relevant location.\n",
    "\n",
    "Steps 4 and 5 above occur in parallel -- meaning, if you have a dataset with 200 scans, there will be 200 EC2 instances running at the same time.\n",
    "\n",
    "For arbitrarily large datasets, full runs generally take a little over an hour, at roughly \\$.05/scan.\n",
    "\n",
    "## Batch Setup : Console\n",
    "\n",
    "This section shows you how to set up a batch environment such that you can easily run ndmg in parallel. For more in-depth documentation, see the [AWS official documentation](https://docs.aws.amazon.com/batch/index.html).\n",
    "\n",
    "### Compute Environment\n",
    "\n",
    "Your compute environment defines the compute resources AWS will use for its instances.\n",
    "\n",
    "For ndmg, we recommend exclusively using `r5.large` instance types.\n",
    "\n",
    "#### Initial Setup\n",
    "\n",
    "**TODO : check if default roles can access s3 <br>**\n",
    "Here, you define who can access the environment. We recommend using the default IAM roles.\n",
    "\n",
    "1. Use Managed\n",
    "2. Define a compute environment name\n",
    "3. Define a service role. The default IAM is best.\n",
    "4. Define an instance role. The default IAM (ecsInstanceRole) is best.\n",
    "5. If you want to connect to your environments, you'll need a key-pair. This is optional; if you want to set this up, see the [official documentation](https://docs.aws.amazon.com/batch/latest/userguide/get-set-up-for-aws-batch.html#create-a-key-pair).\n",
    "\n",
    "![initial compute](https://i.imgur.com/vEmEpuf.png)\n",
    "\n",
    "#### Compute Resources\n",
    "\n",
    "This section lets you define what resources your environment will use. You don't need to use any launch templates here; essentially the only thing necessary is to set the instance type to `r5.large`, and to raise the maximum vCPUs (in case you're running large datasets)\n",
    "\n",
    "1. Set Allowed Instance types to `r5.large`\n",
    "2. Set minimum vCPUs to 0\n",
    "\n",
    "![Compute Resources](https://i.imgur.com/QZh4IlG.png)\n",
    "\n",
    "You can leave the rest of the options set as default in compute environments.\n",
    "\n",
    "### Job Queue\n",
    "\n",
    "Now, attach the compute environment to the job queue.\n",
    "\n",
    "1. Click \"create queue\" in \"Job Queues\"\n",
    "2. Give a queue name\n",
    "3. Give a priority (e.g., `1`)\n",
    "4. Attach the compute environment you just created to the job queue\n",
    "5. Click \"Create Job Queue\"\n",
    "\n",
    "![queue](https://i.imgur.com/wJk8Og0.png)\n",
    "\n",
    "### Job Definition\n",
    "\n",
    "The job definition is where you specify the container, the job role, the vCPUs, and the memory.\n",
    "\n",
    "1. For \"container image\", you can use `neurodata/ndmg_dev:latest`.\n",
    "2. For vCPUs, use 2.\n",
    "3. Because we want to use one r5.large EC2 instance per scan, use `15200` for the memory.\n",
    "\n",
    "You can leave the other parameters empty.\n",
    "\n",
    "![def1](https://i.imgur.com/n1l2lPo.png)\n",
    "\n",
    "## Running the Pipeline\n",
    "\n",
    "Given that you've set up your batch environment properly, and your IAM credentials are set up properly, submitting jobs to batch is relatively simple:\n",
    "\n",
    "        ndmg_cloud participant --bucket <bucket> --bidsdir <path on bucket> --jobdir <local/jobdir> --modif <name-of-s3-directory-output>\n",
    "\n",
    "An example using one of our s3 buckets can be seen below. Note that your dataset must be BIDs-formatted.:\n",
    "\n",
    "        ndmg_cloud participant --bucket ndmg-data --sp native --bidsdir HNU1 --jobdir ~/.ndmg/jobs/HNU1-08-21-native --modif ndmg-08-21-native --dataset HNU1\n",
    "\n",
    "Note that the above example won't work for people without access to our bucket.\n",
    "\n",
    "Behind the scenes, what happens here is:\n",
    "\n",
    "1. `ndmg` parses through your s3 bucket, and gets the locations of all subjects and sessions\n",
    "2. A unique `json` file is created for each scan, containing the ndmg run parameters for that scan.\n",
    "3. Every `json` file is submitted sequentially to `AWS-Batch`, and a job begins that runs that scan.\n",
    "4. Once the scan is done, the output data is pushed to the output location on `s3` specified by `--modif`.\n",
    "\n",
    "## Monitoring Runs\n",
    "\n",
    "You can see all of your runs in the `Batch` Dashboard.\n",
    "\n",
    "![batch-runs](https://i.imgur.com/b9GbKdT.png)\n",
    "\n",
    "You can monitor the outputs of a given run by clicking on it and clicking `View logs for the most recent attempt in the CloudWatch console`.\n",
    "\n",
    "![cloudwatch](https://i.imgur.com/K4djTW1.png)\n",
    "\n",
    "When all of your runs finish, the outputs will on `s3`, and you can use your favorite way to pull your graphs to your local machine!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
